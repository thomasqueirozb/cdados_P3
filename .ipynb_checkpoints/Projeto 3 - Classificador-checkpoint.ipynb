{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 3 de Ciência dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrantes\n",
    "\n",
    "- Bruno Domingues\n",
    "- Stefano Moretti\n",
    "- Thomas Queiroz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introdução\n",
    "\n",
    "O projeto consiste na montagem de um classificador que tem como input uma foto de uma pessoa qualquer, e com base na foto retornar o sexo biológico da pessoa na foto.\n",
    "\n",
    "Para tal, usamos um dataset composto de 13000 fotos, chamado de \"Labeled Faces in the Wild\". Nele, há 13000 fotos de 5749 pessoas diferentes, e dentre estas, 1680 pessoas possuem mais de uma foto. A única \"restrição\" presente é que as fotos foram obtidas através do detector de faces Viola-Jones. Há quatro versões do dataset que podem ser encontradas:\n",
    "- versão original\n",
    "- versão com alinhamento por deep funneling\n",
    "- versão com alinhamento por LFW-a\n",
    "- versão com alinhamento por funneling\n",
    "\n",
    "No caso, optamos pela última, uma vez que acreditamos que o alinhamento das faces torna o processo de detectar padrões mais fácil.\n",
    "\n",
    "Com o dataset em mãos, começamos a adaptá-lo para nosso projeto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpeza dos dados\n",
    "\n",
    "Primeiro, obtivemos o local onde as fotos se encontravam no computador. Em seguida, unimos as fotos com 2 arquivos txt em uma pasta, sendo que estes arquivos continham os nomes de homens e mulheres presentes no dataset, gerando nossos labels. Após isto, começamos a editar as fotos: reduzimos o tamanho da foto de 250x250 para 125x125, depois centralizamos melhor a imagem e reduzimos para 80x80; em seguida, reduzimos para 40x40 pixels. Por fim, transformamos as fotos em listas unidimensionais em que cada pixel representa um elemento, de forma que o classificador possa analisar cada pixel individualmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, os\n",
    "import numpy as np\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ori = os.getcwd()\n",
    "path_dataset = \"C:\\\\Users\\\\thoma\\\\Documents\\\\INSPER\\\\2 Sem\\\\Ciência dos Dados\\\\lfw_funneled\"\n",
    "# path_dataset = \"C:\\\\Users\\\\Bruno\\\\Documents\\\\Insper\\\\2º Semestre\\\\Ciência dos Dados\\\\Projeto_3\\\\lfw_funneled\"\n",
    "# path_dataset = \"C:\\\\Users\\\\Stefano Moretti\\\\Documents\\\\INSPER\\\\2 Sem\\\\Ciência dos Dados\\\\lfw_funneled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tempo para carregar as fotos: 48.31 segundos'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialloadTime = time()\n",
    "\n",
    "os.chdir(path_dataset)\n",
    "\n",
    "with open('male_names.txt','r') as m:\n",
    "    male = m.readlines()\n",
    "    male = [i.replace(\"\\n\",\"\") for i in male]\n",
    "\n",
    "with open('female_names.txt','r') as fm:\n",
    "    female = fm.readlines()\n",
    "    female = [i.replace(\"\\n\",\"\") for i in female]\n",
    "\n",
    "\n",
    "m = []\n",
    "fm = []\n",
    "\n",
    "for pasta in os.listdir(path_dataset):\n",
    "    if \".\" not in pasta:\n",
    "        path_folder = \".\\\\\" + pasta\n",
    "        for im in os.listdir(path_folder):\n",
    "            tempIm = cv2.imread(path_folder + \"\\\\\" + im) # 250 x 250\n",
    "\n",
    "            tempIm = cv2.pyrDown(tempIm) # 125 x 125\n",
    "            tempIm = tempIm[20:100, 20:100] # 80 x 80\n",
    "            tempIm = cv2.pyrDown(tempIm) # 40 x 40\n",
    "            tempIm = cv2.cvtColor(tempIm, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            tempIm = tempIm.flatten()\n",
    "            \n",
    "            if im in male:\n",
    "                m.append(tempIm)\n",
    "            elif im in female:\n",
    "                fm.append(tempIm)\n",
    "\n",
    "os.chdir(path_ori)\n",
    "\n",
    "finalloadTime = time()\n",
    "f'Tempo para carregar as fotos: {finalloadTime - initialloadTime :.2f} segundos'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após a limpeza dos dados, juntamos as fotos de homens e mulheres, e embaralhamos as fotos. Em seguida, separamos o dataset em 2 partes: 70% das fotos seriam usadas para o treino do classificador e o restante seria usado para teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = m + fm\n",
    "y = [0]*len(m) + [1]*len(fm)\n",
    "\n",
    "c = list(zip(X, y))\n",
    "\n",
    "np.random.shuffle(c)\n",
    "\n",
    "X, y = zip(*c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = round(len(X) * 0.7)\n",
    "X_train, X_test = X[:index], X[index:]\n",
    "y_train, y_test = y[:index], y[index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com o dataset de treino e teste separado, pudemos seguir para a escolha de nosso classificador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classificador\n",
    "\n",
    "Após testar diversos classificadores que podem ser encontrados na biblioteca do scikit learn, optamos por usar o Gradient Boosting Classifier, uma vez que ele nos retornou a melhor acurácia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo para treinar o modelo: 467.36 segundos\n",
      "Tempo para testar o modelo: 0.13 segundos\n",
      "Acurácia: 87.53 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "\n",
    "train_model = True\n",
    "\n",
    "if train_model:\n",
    "    comeco_treino = time()\n",
    "    clf = GradientBoostingClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    fim_treino = time()\n",
    "    print(f'Tempo para treinar o modelo: {fim_treino - comeco_treino:.2f} segundos')\n",
    "\n",
    "    comeco_prev = time()\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    fim_prev = time()\n",
    "    print(f'Tempo para testar o modelo: {fim_prev - comeco_prev :.2f} segundos')\n",
    "\n",
    "\n",
    "    print('Acurácia: {0:.2f} %'.format(100 * accuracy_score(y_test, y_pred)))\n",
    "    \n",
    "    # Salvando o modelo\n",
    "    pickle.dump(clf, open('model.p', 'wb'))\n",
    "else:\n",
    "    # Carregando o modelo\n",
    "    clf = pickle.load(open('model.p', 'rb'))\n",
    "    \n",
    "    comeco_prev = time()\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    fim_prev = time()\n",
    "    print(f'Tempo para testar o modelo: {fim_prev - comeco_prev :.2f} segundos')\n",
    "\n",
    "\n",
    "    print('Acurácia: {0:.2f} %'.format(100 * accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mas afinal, o que o Gradient Boosting faz?\n",
    "\n",
    "O Gradient Boosting funciona através do acréscimo sequencial de preditores em um conjunto, de modo que cada um corrija seu antecessor. Este método tenta adaptar o novo preditor para prever os *erros residuais* feitos pelo preditor anterior. Um exemplo disto seria usando árvores de decisão como preditores: imagine que usamos uma árvore que faz um fit do $X_{train}$ e o $y_{train}$. Em seguida, usamos uma segunda árvore de decisão, de modo que ela faça um fit do $X_{train}$ e do erro residual da árvore anterior (dado por $\\hat{y_2}$). Após isso, usamos uma terceira árvore de decisão, que usa ainda o $X_{train}$ e o erro residual da segunda árvore. Com isso, temos um conjunto de três árvores, de modo que a previsão deste conjunto é dada pela soma das previsões de cada árvore. Para decidir o número de árvores mais adequado, pode ser implementado o que é chamado de early stopping, mas optamos por não utilizar uma vez que exigia muito tempo para treinar o modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliografia\n",
    "\n",
    "Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller.\n",
    "**Labeled Faces in the Wild: A Database for Studying Face Recognition in Unconstrained Environments.**\n",
    "University of Massachusetts, Amherst, Technical Report 07-49, October, 2007.\n",
    "http://vis-www.cs.umass.edu/lfw/\n",
    "\n",
    "Aurelien Geron.\n",
    "**Hands–On Machine Learning with Scikit–Learn and TensorFlow**\n",
    "\n",
    "Slides de aula\n",
    "\n",
    "https://github.com/Insper/CD18"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
